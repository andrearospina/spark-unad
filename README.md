# Tarea 3 ‚Äî Procesamiento de Datos con Apache Spark

## üë©‚Äçüíª Estudiante
**Andrea Rodriguez Ospina - UNAD**

---

## üß© 1. Definici√≥n del problema y conjunto de datos

### üìñ Problema a resolver
Netflix cuenta con un extenso cat√°logo de pel√≠culas y series que var√≠a constantemente seg√∫n el pa√≠s, la fecha de lanzamiento y la categor√≠a.  
El objetivo de este proyecto es **analizar el cat√°logo de shows** para identificar:
- Qu√© tipo de contenido predomina (pel√≠culas o series),
- Qu√© pa√≠ses tienen m√°s producciones,
- Cu√°les son los g√©neros m√°s populares.

Este an√°lisis permite extraer informaci√≥n relevante sobre las tendencias del contenido disponible en la plataforma, aplicando t√©cnicas de procesamiento de datos.


### üìÇ Conjunto de datos seleccionado
Se utiliz√≥ el dataset p√∫blico **Netflix Movies and TV Shows**, disponible en [Kaggle](https://www.kaggle.com/datasets/shivamb/netflix-shows).

**Caracter√≠sticas del dataset:**
- Fuente: Kaggle (dataset p√∫blico)
- Atributos: `show_id`, `type`, `title`, `director`, `cast`, `country`, `release_year`, `rating`, `duration`, `listed_in`, `description`
- Ruta local utilizada: /home/andrea-rodriguez/Downloads/netflix_titles.csv

## ‚öôÔ∏è Overview de la actividad 

Durante el desarrollo de esta tarea se configur√≥ un entorno completo para trabajar con **procesamiento de datos masivos** utilizando **Apache Hadoop** y **Apache Spark** en una m√°quina virtual con Ubuntu.

Primero, se valid√≥ el funcionamiento de **Hadoop** mediante los comandos `start-dfs.sh`, `start-yarn.sh` y `jps`.  
Esto permiti√≥ iniciar el sistema de archivos distribuido y acceder a las interfaces web de administraci√≥n (NameNode y ResourceManager).

Posteriormente, se procedi√≥ a la **instalaci√≥n de Spark 4.0.1** (compatible con Hadoop 3), por medio de la la funci√≥n de carpetas compartidas de VirtualBox, configurando las variables de entorno en el archivo `~/.bashrc` y verificando la correcta inicializaci√≥n de Spark con el comando `pyspark`.  
Desde este entorno se desarrollaron dos tipos de procesamiento:

### ‚öôÔ∏è Procesamiento Batch

Se carg√≥ el dataset p√∫blico **Netflix Movies and TV Shows**, aplicando limpieza, transformaciones y un an√°lisis exploratorio b√°sico de datos (EDA) utilizando **Spark DataFrames**.

**Resultados principales:**
- Total de registros: **8,807**
- Pel√≠culas (Movies): **6,131**
- Series (TV Shows): **2,676**
- Pa√≠ses con m√°s producciones: **Estados Unidos, India y Reino Unido**
- G√©neros m√°s comunes: **International Movies, Dramas, Comedies, Documentaries**

Los resultados procesados fueron exportados en formato **CSV** a la ruta local `/home/andrea-rodriguez/Downloads/resultados_netflix`.

### üîÑ Procesamiento en Tiempo Real (Streaming)

Se implement√≥ un flujo de **Spark Structured Streaming** para simular la llegada de datos en tiempo real, utilizando una carpeta local como fuente (`/home/andrea-rodriguez/stream_input`).

- Se procesaban los datos conforme eran a√±adidos a la carpeta.  
- Se contaban los registros por tipo de contenido (pel√≠culas o series).  
- Los resultados se mostraban en consola en tiempo real.  
- No se utiliz√≥ **Kafka** debido a limitaciones de memoria de mi computador; pero el streaming local cumpli√≥ la misma funci√≥n de demostraci√≥n.

---
### üîπ Problemas y Soluciones

| **Problema** | **Causa** | **Soluci√≥n** |
|---------------|------------|---------------|
| Error 404 al descargar Spark | URL oficial no disponible | Descarga alternativa desde `archive.apache.org` y usar la opci√≥n de carpetas compartidas de virtualbox |
| Permiso denegado en `/usr/local/spark` | Carpeta de root | `sudo chown -R $USER:$USER /usr/local/spark` |
| Incompatibilidad con Java 11 | Spark 4.0 requiere Java 17 | Instalaci√≥n de OpenJDK 17 |
| Fallo Spark + Kafka - Colapso de Virtualbox| Limitaci√≥n de memoria RAM | Se reemplaz√≥ por una simulaci√≥n local con carpeta de streaming |
| Interfaz Spark UI no abre | Spark detenido | Reiniciar PySpark y acceder nuevamente a http://localhost:4040 |

---
### üåê Verificaci√≥n y Monitoreo

- Se comprob√≥ el correcto funcionamiento de Spark mediante su interfaz web:  
  **http://bd-ubuntu:4040**
- Se valid√≥ la escritura de los resultados en el sistema local.
- Se detuvieron los servicios correctamente para conservar la configuraci√≥n del entorno.
- El c√≥digo que se ejecut√≥ en consola esta en el repositorio como: `spark-andrea-rodriguez.py`
---

### üéØ Conclusi√≥n

Este proyecto me permiti√≥ comprender como funcionan los ciclos de procesamiento de datos con **Apache Spark**, desde su instalaci√≥n y configuraci√≥n hasta la ejecuci√≥n de tareas **batch** y **streaming** en un entorno distribuido. 

